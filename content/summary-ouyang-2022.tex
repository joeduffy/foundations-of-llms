% Paper Summary: Training Language Models to Follow Instructions with Human Feedback (Ouyang et al., 2022)

\begin{papersummary}{2022}{Training Language Models to Follow Instructions with Human Feedback}{Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe}{InstructGPT demonstrated that RLHF transforms base language models into helpful, harmless, and honest assistants that follow instructions, establishing the alignment paradigm used by all modern LLM chatbots.}

\summaryheading{Key Ideas}
InstructGPT fine-tunes GPT-3 using RLHF to align model behavior with human preferences, scaling the approach first demonstrated on GPT-2 by \hyperref[paper:ziegler-2019]{Ziegler et al.\ (2019)}. The process involves collecting demonstrations of desired behavior, training a reward model on human preference comparisons, and using PPO to optimize the policy against the learned reward while maintaining proximity to the original model through a KL penalty. The result is a model that better follows instructions, refuses inappropriate requests, and produces more helpful outputs. Despite being smaller than GPT-3, InstructGPT outputs are strongly preferred by humans, demonstrating that alignment matters more than raw scale.

\summaryheading{Follow-on Works}
ChatGPT applied InstructGPT's approach at scale, catalyzing widespread LLM adoption. GPT-4, Claude, Gemini, and all major LLM chatbots use RLHF or variants. Constitutional AI, RLAIF, and DPO refined the alignment process. The instruction-following paradigm established by InstructGPT became universal for LLM interaction. Modern alignment research builds directly on this foundation.

\summaryheading{Lasting Contributions}
InstructGPT established RLHF as the standard method for aligning LLMs with human values and making them useful as assistants. Every major LLM chatbot employs RLHF or closely related techniques. The work proved that models trained purely for next-token prediction need alignment to be helpful and safe. The transformation from base GPT-3 to InstructGPT demonstrated that alignment is critical for practical deployment. Modern LLM safety and helpfulness stems from techniques introduced here, making InstructGPT foundational to the current era of AI assistants.

\end{papersummary}
