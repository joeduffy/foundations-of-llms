% Paper Summary: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (Shoeybi et al., 2019)

\begin{papersummary}{2019}{Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}{Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro}{Megatron-LM introduced efficient intra-layer tensor parallelism for Transformers, enabling the training of multi-billion parameter language models across GPUs without pipeline bubbles or excessive communication overhead.}

\summaryheading{Key Ideas}
Megatron-LM demonstrated that Transformer layers can be partitioned across GPUs using simple, mathematically principled splits of the weight matrices. For the self-attention block, the key, query, and value projections are split column-wise across GPUs so each GPU computes local attention over its subset of heads, requiring only a single all-reduce for the output projection. For the feedforward block, the first linear layer is split column-wise and the second row-wise, again requiring only one all-reduce per layer. This intra-layer \emph{tensor parallelism} approach avoids the pipeline bubbles inherent in inter-layer (pipeline) parallelism and scales efficiently to 8-way parallelism within a single DGX node connected by NVLink. Using this technique, the authors trained Transformer language models up to 8.3 billion parameters---the largest at the time---achieving state-of-the-art perplexity on WikiText-103 and competitive downstream task accuracy.

\summaryheading{Follow-on Works}
The Megatron framework was extended with pipeline parallelism to form the ``3D parallelism'' paradigm (data + tensor + pipeline) used to train models at the GPT-3 scale and beyond. NVIDIA's Megatron-Core became the reference training infrastructure adopted by numerous organizations. DeepSpeed integrated complementary memory optimizations (ZeRO) that compose with Megatron-style tensor parallelism. GPT-3 (\hyperref[paper:brown-2020]{Brown et al., 2020}) relied on model parallelism to distribute its 175 billion parameters during training. Subsequent systems including PaLM (\hyperref[paper:chowdhery-2022]{Chowdhery et al., 2022}), LLaMA (\hyperref[paper:touvron-2023-llama]{Touvron et al., 2023}), and DeepSeek-V3 all use parallelism strategies descended from this work.

\summaryheading{Lasting Contributions}
Tensor parallelism as formulated in Megatron-LM remains the standard approach for distributing Transformer training across GPUs within a node. Every frontier language model uses some form of model parallelism, and the specific column/row partitioning scheme introduced here is still the dominant pattern. The paper established that systems-level innovation is as essential as architectural innovation for scaling language models: without efficient parallelism, the scaling laws documented by Kaplan et al.\ would have remained theoretical. The Megatron-LM codebase continues to serve as a foundation for large-scale training infrastructure at NVIDIA and elsewhere.

\end{papersummary}
