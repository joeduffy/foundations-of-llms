% Paper Summary: Auto-Encoding Variational Bayes (Kingma & Welling, 2013)

\begin{papersummary}{2013}{Auto-Encoding Variational Bayes}{Diederik P. Kingma, Max Welling}{The Variational Autoencoder introduced the reparameterization trick enabling backpropagation through stochastic nodes, establishing a foundational framework for deep generative modeling.}

\summaryheading{Key Ideas}
The paper introduced a practical algorithm for training latent variable models with neural network encoders and decoders. The encoder maps inputs to parameters of an approximate posterior distribution over latent variables; the decoder reconstructs inputs from sampled latents. The key innovation---the ``reparameterization trick''---expresses samples from the approximate posterior as a deterministic transformation of a noise variable, enabling gradient-based optimization through the sampling process. The evidence lower bound (ELBO) provides a tractable training objective combining reconstruction loss and KL divergence regularization. This framework enables learning rich latent representations for complex data.

\summaryheading{Follow-on Works}
Conditional VAEs enabled structured generation. $\beta$-VAE explored disentangled representations. VQ-VAE introduced discrete latents, influencing later work on tokenization. Hierarchical VAEs scaled to high-resolution images. The latent diffusion model underlying Stable Diffusion uses VAE-encoded latent spaces. Flow-based models extended the framework with invertible transformations. The reparameterization trick influenced the design of many subsequent probabilistic neural networks.

\summaryheading{Lasting Contributions}
The VAE established deep generative modeling as a major research direction, accumulating over 30,000 citations. The reparameterization trick---enabling gradient descent through stochastic nodes---became a fundamental technique used far beyond VAEs, including in reinforcement learning (policy gradient estimators) and attention mechanisms (Gumbel-softmax). While diffusion models have surpassed VAEs for image generation, VAE principles remain relevant: the latent space of Stable Diffusion uses a VAE encoder/decoder, and understanding of reconstruction-regularization trade-offs informs many architectural decisions. The ELBO objective provides foundational intuition for variational inference throughout machine learning.

\end{papersummary}
