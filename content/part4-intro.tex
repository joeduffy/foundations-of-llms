\chapter*{Introduction to Part IV}
\addcontentsline{toc}{chapter}{Introduction to Part IV}

The years 2017 to 2019 produced the two innovations that define contemporary language modeling: the transformer architecture and the pretraining paradigm. Together, they replaced the patchwork of task-specific models that had characterized natural language processing with a single recipe---pretrain a large transformer on unlabeled text, then adapt it to downstream tasks---that proved effective across virtually every language understanding and generation benchmark.

The transformer, introduced by Vaswani and colleagues, replaced recurrence with self-attention: each position in a sequence attends directly to every other position through learned query-key-value projections, computing a weighted combination of value vectors. This eliminates the sequential bottleneck of recurrent architectures, allowing all positions to be processed in parallel during training. The resulting model scales efficiently to long sequences and large batch sizes, making it well suited to modern GPU and TPU hardware. Layer normalization, developed by Ba, Kiros, and Hinton the year before, provided the per-example normalization that proved more appropriate than batch normalization for sequence models and became a standard transformer component. Shazeer's sparsely-gated mixture-of-experts layer introduced conditional computation, allowing model capacity to grow without proportional increases in per-example computation.

The pretraining revolution proceeded along two complementary paths. Peters showed with ELMo that contextualized word representations from pretrained bidirectional LSTMs dramatically improved downstream task performance. Devlin's BERT demonstrated that transformers pretrained with masked language modeling could learn rich bidirectional representations, achieving state-of-the-art results across diverse benchmarks. Radford's GPT-1 established the alternative: a decoder-only transformer pretrained autoregressively on next-token prediction served as a powerful general-purpose feature extractor. Raffel's T5 unified all NLP tasks into a text-to-text format, demonstrating that a single architecture with consistent input-output formatting could handle any language task.

Simultaneously, this period saw the development of reinforcement learning infrastructure that would later prove critical for aligning language models with human preferences. Schulman's Proximal Policy Optimization provided a stable and efficient policy gradient algorithm. Christiano and colleagues demonstrated that agents could learn complex behaviors from human preference comparisons rather than hand-specified reward functions---the paradigm that would become reinforcement learning from human feedback.

\section*{Papers in This Section}

\begin{enumerate}
\item \textbf{Ba, Kiros \& Hinton (2016)} --- Introduced layer normalization for stable training of recurrent and attention-based models.

\item \textbf{Vaswani et al. (2017)} --- The transformer: self-attention replaces recurrence, enabling parallel processing and long-range dependency modeling.

\item \textbf{Shazeer et al. (2017)} --- Sparsely-gated mixture-of-experts layers for conditional computation at scale.

\item \textbf{Schulman et al. (2017)} --- Proximal Policy Optimization, the policy gradient algorithm later used for RLHF training.

\item \textbf{Christiano et al. (2017)} --- Learning from human preferences: reinforcement learning without hand-specified reward functions.

\item \textbf{Peters et al. (2018)} --- ELMo: contextualized word representations from pretrained bidirectional language models.

\item \textbf{Devlin et al. (2018)} --- BERT: bidirectional pretraining through masked language modeling for transfer learning.

\item \textbf{Radford et al. (2018)} --- GPT-1: decoder-only autoregressive pretraining as general-purpose feature extraction.

\item \textbf{Raffel et al. (2019)} --- T5: a unified text-to-text framework for all NLP tasks using a single encoder-decoder transformer.
\end{enumerate}
