Production systems integrate foundational techniques into deployable models, demonstrating how research advances translate into frontier capabilities. This appendix presents technical reports and system cards from leading organizations, documenting how combinations of pretraining, fine-tuning, reinforcement learning from feedback, and architectural innovations yield state-of-the-art performance.

\subsection*{Key Themes}

\textbf{Multimodal Integration:} GPT-4 and Gemini 1.5 demonstrate unified architectures processing text, images, and other modalities, with Gemini achieving context windows exceeding one million tokens.

\textbf{Constitutional AI at Scale:} The Claude 3 family applies constitutional training principles to production systems, demonstrating that safety-focused training can coexist with strong capabilities.

\textbf{Efficient Open Models:} DeepSeek-V3 and Qwen2.5 show that open models can achieve competitive performance through careful data curation, mixture-of-experts architectures, and novel attention mechanisms.

\textbf{Inference-Time Reasoning:} The o1 and o3 systems illustrate evolution toward systems that deliberately allocate inference-time computation for complex reasoning, representing a shift from pure language modeling.

\textbf{Post-Training Synthesis:} Comprehensive surveys document the rapidly evolving landscape of fine-tuning, reinforcement learning, and alignment techniques.

\textbf{Reports in this appendix:}
\begin{itemize}
    \item \textbf{OpenAI (2023)}: GPT-4 Technical Report---multimodal transformer achieving strong performance across diverse benchmarks.
    \item \textbf{Anthropic (2024)}: Claude 3 Model Family---Constitutional AI applied to production systems at scale.
    \item \textbf{DeepMind (2024)}: Gemini 1.5---multimodal model with extended context capability exceeding 1 million tokens.
    \item \textbf{Qwen Team (2024)}: Qwen2.5---open multilingual model series with extensive post-training optimization.
    \item \textbf{DeepSeek-AI (2024)}: DeepSeek-V3---671B mixture-of-experts with auxiliary-loss-free routing and Multi-Head Latent Attention.
    \item \textbf{OpenAI (2024)}: o1 System Card---reinforcement learning for extended chain-of-thought reasoning.
    \item \textbf{Dubey et al. (2024)}: Llama 3---comprehensive open model family with state-of-the-art performance across scales.
    \item \textbf{Kumar et al. (2025)}: LLM Post-Training Survey---comprehensive review of fine-tuning, reinforcement learning, and alignment techniques.
\end{itemize}
