\chapter*{Introduction to Part V}
\addcontentsline{toc}{chapter}{Introduction to Part V}

Between 2019 and 2020, researchers discovered that transformer language models exhibited a remarkable empirical property: performance improved predictably as a function of model size, dataset size, and computational budget, following power-law relationships that held across multiple orders of magnitude. More striking still, sufficiently large models displayed qualitatively new behaviors---zero-shot and few-shot task performance---that smaller models of the same architecture did not possess.

GPT-2 provided the first clear evidence. Trained on 40 GB of web text with 1.5 billion parameters, it performed competitively on language modeling benchmarks without task-specific fine-tuning and could generate remarkably coherent long-form text. OpenAI's decision to initially withhold the full model, citing concerns about misuse, marked the first time a language model's capabilities were considered potentially dangerous. GPT-3 extended these findings to 175 billion parameters and revealed in-context learning: the model could adapt to new tasks from a handful of examples provided in the prompt, without any parameter updates. This capability had no precedent in machine learning and suggested that very large language models had internalized something more general than statistical patterns over token sequences.

Kaplan and colleagues systematized these observations into scaling laws---empirical power-law relationships predicting test loss as a function of model parameters, dataset tokens, and training compute. These laws implied that performance improvements were predictable and that the field had not yet reached diminishing returns, establishing a research paradigm centered on training progressively larger models. Crucially, scaling required not just algorithmic ideas but systems infrastructure: Megatron-LM introduced tensor parallelism for Transformers, partitioning weight matrices across GPUs within a node to train models of unprecedented size, and established the template for distributed training that all frontier models would follow.

Multi-query attention reduced inference memory bandwidth by sharing key-value heads across all attention heads, an optimization that would become standard in production deployments. Lewis and colleagues introduced retrieval-augmented generation, showing that grounding language models in external knowledge retrieval improved factual accuracy without increasing parameter count. Shazeer also demonstrated that Gated Linear Unit variants---particularly SwiGLU---consistently outperformed the standard ReLU feedforward layers in Transformers, an architectural refinement adopted by nearly every subsequent open-weight model.

\section*{Papers in This Section}

\begin{enumerate}
\item \textbf{Radford et al. (2019)} --- GPT-2: large-scale language modeling with zero-shot task transfer.

\item \textbf{Shazeer (2019)} --- Multi-query attention for faster inference through shared key-value heads.

\item \textbf{Shoeybi et al. (2019)} --- Megatron-LM: tensor parallelism for training multi-billion parameter Transformers across GPUs.

\item \textbf{Brown et al. (2020)} --- GPT-3: few-shot in-context learning at 175 billion parameters.

\item \textbf{Kaplan et al. (2020)} --- Scaling laws: power-law relationships governing model performance as a function of size, data, and compute.

\item \textbf{Lewis et al. (2020)} --- Retrieval-augmented generation combining parametric models with non-parametric knowledge retrieval.

\item \textbf{Shazeer (2020)} --- GLU variants (SwiGLU) as improved feedforward activations for Transformers.

\end{enumerate}
