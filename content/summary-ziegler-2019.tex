% Paper Summary: Fine-Tuning Language Models from Human Preferences (Ziegler et al., 2019)

\begin{papersummary}{2019}{Fine-Tuning Language Models from Human Preferences}{Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving}{This paper was the first to apply reinforcement learning from human preferences directly to language models, fine-tuning GPT-2 on stylistic continuation and summarization tasks using a learned reward model optimized with PPO.}

\summaryheading{Key Ideas}
The paper transfers the RLHF framework of \hyperref[paper:christiano-2017-rlhf]{Christiano et al.\ (2017)}---which demonstrated reward learning from human comparisons in Atari and MuJoCo environments---to natural language generation. Starting from a 774M-parameter GPT-2 model, the authors train a reward model on human preference comparisons between model outputs, then optimize the language model policy against the learned reward using PPO with a KL penalty that prevents the fine-tuned model from drifting too far from the pretrained distribution. This KL-constrained optimization became the standard recipe for all subsequent RLHF work on language models. The approach is evaluated on four tasks: continuing text with positive sentiment, continuing text with physically descriptive language, and summarization on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation, only 5,000 human comparisons suffice to produce outputs preferred by humans 86\% of the time over the base model. For summarization, models trained with 60,000 comparisons learn to extract relevant content, though they behave as ``smart copiers'' rather than generating abstractive summaries.

\summaryheading{Follow-on Works}
Stiennon et al.\ (2020) scaled this approach to larger models and more data for summarization, achieving human-level performance. \hyperref[paper:ouyang-2022]{InstructGPT (Ouyang et al., 2022)} applied the same reward-model-then-PPO pipeline to GPT-3, producing the first instruction-following assistant and demonstrating that RLHF alignment matters more than raw scale. \hyperref[paper:bai-2022-constitutional]{Constitutional AI (Bai et al., 2022)} extended the paradigm with AI-generated feedback. \hyperref[paper:rafailov-2023]{DPO (Rafailov et al., 2023)} later simplified the pipeline by eliminating the explicit reward model, but the KL-constrained optimization objective traces directly to this work.

\summaryheading{Lasting Contributions}
This paper established the concrete methodology---reward model trained on human comparisons, KL-penalized PPO fine-tuning of a pretrained language model---that became the standard alignment recipe for GPT-4, Claude, Gemini, and all major production LLMs. It demonstrated that RLHF, previously validated only in game and robotics environments, could meaningfully improve language model outputs as judged by humans. The KL penalty formulation introduced here (modifying the reward with $\beta \log \frac{\pi(y|x)}{\rho(y|x)}$) became ubiquitous in subsequent work and is the starting point from which DPO's closed-form solution was derived. Without this paper, the path from Christiano et al.'s proof of concept to InstructGPT's production-scale deployment would lack its essential intermediate validation.

\end{papersummary}
