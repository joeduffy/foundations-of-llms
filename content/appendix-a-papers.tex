% Appendix A Papers: Emerging Results (2023â€“2025)

\addcontentsline{toc}{subsection}{[2023] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Gu \& Dao)}
\phantomsection
\label{paper:gu-dao-2023}
\input{content/summary-gu-dao-2023}
\includepdf[pages=-,pagecommand={}]{pdfs/gu-dao-2023-mamba.pdf}

\addcontentsline{toc}{subsection}{[2023] Sparse Autoencoders Find Highly Interpretable Features in Language Models (Cunningham et al.)}
\phantomsection
\label{paper:cunningham-2023}
\input{content/summary-cunningham-2023}
\includepdf[pages=-,pagecommand={}]{pdfs/cunningham-2023.pdf}

\addcontentsline{toc}{subsection}{[2024] MoE Scaling Laws and Expert Choice Routing (Frantar et al.)}
\phantomsection
\label{paper:frantar-2024}
\input{content/summary-frantar-2024}
\includepdf[pages=-,pagecommand={}]{pdfs/frantar-2024.pdf}

\addcontentsline{toc}{subsection}{[2025] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention (Yuan et al.)}
\phantomsection
\label{paper:yuan-2025-nsa}
\input{content/summary-yuan-2025-nsa}
\includepdf[pages=-,pagecommand={}]{pdfs/yuan-2025-nsa.pdf}

\addcontentsline{toc}{subsection}{[2025] Gated Attention for Large Language Models (Qiu et al.)}
\phantomsection
\label{paper:qiu-2025-gated-attention}
\input{content/summary-qiu-2025-gated-attention}
\includepdf[pages=-,pagecommand={}]{pdfs/qiu-2025-gated-attention.pdf}
