The years 2021-2022 focused on making large language models more efficient, aligned with human values, and capable of complex reasoning. The field recognized that raw scale alone was insufficientâ€”models needed to be deployable, controllable, and capable of systematic reasoning.

Efficiency improvements addressed practical deployment challenges. \hyperref[paper:hu-2021]{LoRA (Hu et al., 2021)} demonstrated that large models could be efficiently adapted by training only low-rank decompositions, making fine-tuning accessible and practical for resource-constrained environments. Architectural improvements included \hyperref[paper:su-2021]{RoFormer (Su et al., 2021)} with rotary position embeddings and \hyperref[paper:press-2021]{ALiBi (Press et al., 2021)}, which enabled extrapolation to longer sequences without retraining. \hyperref[paper:dao-2022]{FlashAttention (Dao et al., 2022)} provided fast, memory-efficient exact attention computation, while \hyperref[paper:fedus-2022]{Switch Transformers (Fedus et al., 2022)} demonstrated trillion-parameter sparse models where each token activates only a fraction of the model's capacity.

Knowledge integration was advanced through \hyperref[paper:borgeaud-2021]{RETRO (Borgeaud et al., 2021)}, which scaled retrieval-augmented models to trillions of tokens, and \hyperref[paper:hoffmann-2022]{Chinchilla (Hoffmann et al., 2022)}, which established compute-optimal training laws showing that many models were undertrained relative to their size.

Alignment breakthroughs made models more helpful and harmless. \hyperref[paper:ouyang-2022]{InstructGPT (Ouyang et al., 2022)} demonstrated that reinforcement learning from human feedback (RLHF) could align models with human preferences, producing more helpful and harmless outputs. \hyperref[paper:bai-2022]{Constitutional AI (Bai et al., 2022)} introduced training methods using AI feedback to improve harmlessness while reducing reliance on human annotation.

Reasoning capabilities emerged through prompting innovations. \hyperref[paper:wei-2022]{Chain-of-thought prompting (Wei et al., 2022)} showed that proper prompting could elicit step-by-step reasoning, dramatically improving performance on complex tasks. \hyperref[paper:yao-2022]{ReAct (Yao et al., 2022)} created a framework combining reasoning and acting for complex task solving, demonstrating how language models could interleave thought and action in problem-solving scenarios.

\newpage

% Papers follow directly after the introduction

\addcontentsline{toc}{subsection}{[2021] LoRA: Low-Rank Adaptation of Large Language Models (Hu et al.)}
\phantomsection
\label{paper:hu-2021}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/hu-2021.pdf}

\addcontentsline{toc}{subsection}{[2021] RoFormer: Enhanced Transformer with Rotary Position Embedding (Su et al.)}
\phantomsection
\label{paper:su-2021}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/su-2021.pdf}

\addcontentsline{toc}{subsection}{[2021] Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation (Press et al.)}
\phantomsection
\label{paper:press-2021}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/press-2021.pdf}

\addcontentsline{toc}{subsection}{[2021] Improving Language Models by Retrieving from Trillions of Tokens (Borgeaud et al.)}
\phantomsection
\label{paper:borgeaud-2021}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/borgeaud-2021.pdf}

\addcontentsline{toc}{subsection}{[2022] Switch Transformer: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Fedus et al.)}
\phantomsection
\label{paper:fedus-2022}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/fedus-2022.pdf}

\addcontentsline{toc}{subsection}{[2022] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Dao et al.)}
\phantomsection
\label{paper:dao-2022}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/dao-2022.pdf}

\addcontentsline{toc}{subsection}{[2022] Training Language Models to Follow Instructions with Human Feedback (Ouyang et al.)}
\phantomsection
\label{paper:ouyang-2022}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/ouyang-2022.pdf}

\addcontentsline{toc}{subsection}{[2022] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al.)}
\phantomsection
\label{paper:wei-2022}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/wei-2022.pdf}

\addcontentsline{toc}{subsection}{[2022] Constitutional AI: Harmlessness from AI Feedback (Bai et al.)}
\phantomsection
\label{paper:bai-2022}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/bai-2022.pdf}

\addcontentsline{toc}{subsection}{[2022] ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al.)}
\phantomsection
\label{paper:yao-2022}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/yao-2022.pdf}

\addcontentsline{toc}{subsection}{[2022] Training Compute-Optimal Large Language Models (Hoffmann et al.)}
\phantomsection
\label{paper:hoffmann-2022}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/hoffmann-2022.pdf}