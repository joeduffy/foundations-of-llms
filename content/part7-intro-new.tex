The release of openly accessible foundation models democratized research and development in large language models. Meta's LLaMA series demonstrated that smaller, efficiently trained models could achieve performance competitive with significantly larger proprietary systems when trained on high-quality data with appropriate compute budgets.

Alignment techniques evolved beyond reinforcement learning from human feedback. Rafailov et al.'s Direct Preference Optimization reformulated the RLHF objective to eliminate the explicit reward model and policy gradient optimization, deriving a simpler classification loss directly from preference data. This approach reduces training complexity while maintaining alignment quality.

Lee et al.'s work on Reinforcement Learning from AI Feedback demonstrated that large language models themselves could provide preference labels at scale, reducing dependence on costly human annotation. Yuan et al. extended this concept with self-rewarding language models, where the model iteratively improves both its generation and evaluation capabilities through self-generated feedback loops.

Architectural innovations addressed the quadratic complexity bottleneck of attention mechanisms. Liu et al.'s Ring Attention enables processing of sequences exceeding millions of tokens through blockwise computation distributed across devices. Munkhdalai et al.'s Infini-attention integrates compressive memory into standard attention, achieving effective infinite context through a combination of local attention and long-term linear attention mechanisms.

DeepSeek-V2 introduced Multi-Head Latent Attention and refined mixture-of-experts routing, achieving superior inference efficiency through KV cache compression and auxiliary-loss-free load balancing. The Llama 3 family established new benchmarks for open models across diverse tasks, demonstrating sophisticated capabilities in reasoning, coding, and multilingual understanding.

Research into test-time compute scaling revealed that inference-time deliberation can yield performance improvements competitive with scaling model parameters. Snell et al. demonstrated that allocating additional computation during inference through techniques like best-of-N sampling and process-based verification enables smaller models to approach or exceed the performance of larger models on challenging reasoning tasks.

\textbf{Papers in this section:}
\begin{itemize}
    \item \textbf{Touvron et al. (2023)}: LLaMA---openly accessible foundation models trained with compute-efficient scaling.
    \item \textbf{Rafailov et al. (2023)}: Direct Preference Optimization---alignment through preference classification without explicit rewards.
    \item \textbf{Lee et al. (2023)}: RLAIF---reinforcement learning from AI-generated preference labels.
    \item \textbf{Liu et al. (2023)}: Ring Attention---blockwise attention enabling near-infinite context through distributed computation.
    \item \textbf{Dettmers et al. (2023)}: QLoRA---parameter-efficient finetuning through quantization.
    \item \textbf{Munkhdalai et al. (2024)}: Infini-attention---compressive memory for effective infinite context.
    \item \textbf{Yuan et al. (2024)}: Self-Rewarding Language Models---iterative self-improvement through self-generated feedback.
    \item \textbf{Touvron et al. (2024)}: The Llama 3 Herd---open models achieving frontier capabilities across diverse tasks.
    \item \textbf{Shao et al. (2024)}: DeepSeekMath---mathematical reasoning through Group Relative Policy Optimization.
    \item \textbf{Zhu et al. (2024)}: DeepSeek-V2---Multi-Head Latent Attention and auxiliary-loss-free mixture-of-experts.
    \item \textbf{Mistral AI (2024)}: Mixtral---sparse mixture-of-experts achieving strong performance with selective activation.
    \item \textbf{Snell et al. (2024)}: Scaling Test-Time Compute---inference-time deliberation as alternative to parameter scaling.
\end{itemize}
