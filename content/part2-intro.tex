\chapter*{Introduction to Part II}
\addcontentsline{toc}{chapter}{Introduction to Part II}

The period from 1997 to 2013 transformed natural language processing from a field dominated by hand-engineered features and symbolic grammars into one built on learned representations and statistical sequence models. The central technical problem was how to represent words and sentences as mathematical objects amenable to gradient-based optimization---and how to model the sequential dependencies that give language its structure.

Recurrent neural networks offered a natural framework for sequence modeling, but training them proved difficult. Gradients propagated through many time steps either vanished or exploded, preventing networks from learning long-range dependencies. Hochreiter and Schmidhuber resolved this with the LSTM architecture, which introduced gated memory cells that could selectively retain or discard information across arbitrary time intervals. The LSTM made recurrent networks practical for language modeling, speech recognition, and machine translation.

Simultaneously, researchers developed methods for mapping discrete words into continuous vector spaces. Bengio's neural probabilistic language model jointly learned word embeddings and a language model, demonstrating that distributed representations could capture semantic regularities. A decade later, Mikolov showed that simple log-linear models trained on large corpora produced word vectors exhibiting regular algebraic structure---the vector offset from ``king'' to ``queen'' closely approximated that from ``man'' to ``woman.'' These word embeddings became ubiquitous as input representations for downstream tasks.

Underpinning these advances were foundational contributions in optimization and representation learning. Glorot and Bengio's analysis of initialization schemes enabled stable training of deeper networks. Collobert and Weston demonstrated that neural networks trained on unlabeled text with minimal supervision could match hand-engineered NLP systems. Graves showed that recurrent networks could generate coherent sequences character by character, presaging the generative capabilities that would later define large language models.

\section*{Papers in This Section}

\begin{enumerate}
\item \textbf{Hochreiter \& Schmidhuber (1997)} --- Introduced LSTM networks with gated memory cells, solving the vanishing gradient problem for sequence modeling.

\item \textbf{Bengio et al. (2003)} --- Proposed neural language modeling with jointly learned word embeddings and probability distributions over sequences.

\item \textbf{Glorot \& Bengio (2010)} --- Analyzed training dynamics in deep networks and proposed normalized initialization for stable gradient flow.

\item \textbf{Collobert et al. (2011)} --- Showed that neural networks could learn competitive NLP representations from unlabeled data without task-specific feature engineering.

\item \textbf{Mikolov et al. (2013)} --- Developed efficient word embedding algorithms (Word2Vec) that revealed algebraic structure in learned vector spaces.

\item \textbf{Graves (2013)} --- Demonstrated neural sequence generation, showing that recurrent networks could produce coherent handwriting and text.
\end{enumerate}
