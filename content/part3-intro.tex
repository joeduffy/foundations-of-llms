\chapter*{Introduction to Part III}
\addcontentsline{toc}{chapter}{Introduction to Part III}

Between 2012 and 2015, two developments converged to make large-scale language modeling feasible: the deep learning revolution in computer vision established that neural networks could be trained at unprecedented scale using GPU acceleration, and the invention of the attention mechanism removed the information bottleneck that had constrained sequence-to-sequence models.

The deep learning revolution began with AlexNet's performance on ImageNet in 2012, which demonstrated that convolutional networks trained on GPUs could surpass all prior approaches to visual recognition. This result validated decades of theoretical work on deep architectures and triggered rapid progress in network design. Batch normalization addressed the internal covariate shift that made deep networks difficult to train. ResNet's skip connections solved the degradation problem in very deep networks by allowing gradients to flow through shortcut paths---a design principle that would later appear in every transformer. The Adam optimizer provided adaptive per-parameter learning rates that became the default for training large models. Dropout offered a practical regularization method grounded in ensemble averaging.

In parallel, the sequence-to-sequence paradigm established the encoder-decoder framework for mapping variable-length inputs to variable-length outputs through a fixed-dimensional bottleneck. Cho introduced the GRU, a simplified recurrent unit, and formalized the encoder-decoder architecture. Sutskever demonstrated that this framework could learn complex transductions end-to-end. Bahdanau then identified the critical weakness of fixed-length encodings---they forced the encoder to compress all source information into a single vector---and proposed the attention mechanism, which allowed the decoder to selectively attend to different portions of the input at each generation step. Byte-pair encoding provided the subword tokenization scheme that balanced vocabulary coverage with computational tractability, becoming the standard input representation for language models.

\section*{Papers in This Section}

\begin{enumerate}
\item \textbf{Krizhevsky et al. (2012)} --- AlexNet demonstrated deep convolutional networks trained on GPUs achieving breakthrough visual recognition.

\item \textbf{Srivastava et al. (2014)} --- Introduced dropout regularization, preventing overfitting by randomly zeroing activations during training.

\item \textbf{Kingma \& Ba (2014)} --- The Adam optimizer combined momentum with adaptive per-parameter learning rates.

\item \textbf{Cho et al. (2014)} --- Proposed the GRU and formalized the encoder-decoder architecture for sequence-to-sequence learning.

\item \textbf{Sutskever et al. (2014)} --- Demonstrated end-to-end sequence-to-sequence learning with deep LSTMs for machine translation.

\item \textbf{Bahdanau et al. (2014)} --- Introduced the attention mechanism, allowing decoders to selectively focus on relevant source positions.

\item \textbf{Pennington et al. (2014)} --- GloVe combined global co-occurrence statistics with local context windows for word embeddings.

\item \textbf{Ioffe \& Szegedy (2015)} --- Batch normalization stabilized deep network training by normalizing layer inputs.

\item \textbf{He et al. (2015)} --- ResNet introduced skip connections, enabling training of networks with over 100 layers.

\item \textbf{Sennrich et al. (2015)} --- Byte-pair encoding provided efficient subword tokenization balancing coverage and granularity.
\end{enumerate}
