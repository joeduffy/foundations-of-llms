% Section Introductions for "The Foundations of Large Language Models, 1906 - 2025"
% These are technical, focused introductions in the style of a Springer Verlag textbook

\section*{Part 0 – Probabilistic \& Mathematical Foundations (1906–1943)}

The foundations of modern language models rest upon probabilistic frameworks developed in the early twentieth century. Andrey Markov's pioneering work on stochastic processes introduced the concept of state transitions governed by probability distributions---a mathematical abstraction that would prove fundamental to sequence modeling. Markov chains provide the theoretical basis for understanding how systems evolve through discrete states, where the probability of transitioning to the next state depends only on the current state. This memoryless property, known as the Markov assumption, underpins contemporary language modeling, where words or tokens are generated conditionally based on preceding context.

McCulloch and Pitts's 1943 formalization of artificial neurons established the computational primitive from which all modern neural architectures descend. Their binary threshold units, though limited in expressiveness, demonstrated that logical operations could be implemented through networks of simple processing elements. This work laid the conceptual groundwork for understanding computation as an emergent property of interconnected units---a paradigm that would eventually scale to the billion-parameter models of today.

\textbf{Papers in this section:}
\begin{itemize}
    \item \textbf{Markov (1906-1913)}: Markov Chains and Stochastic Processes---introduced probabilistic state transitions fundamental to sequence modeling.
    \item \textbf{McCulloch \& Pitts (1943)}: A Logical Calculus of Ideas---formalized artificial neurons as binary threshold units capable of logical computation.
\end{itemize}

\section*{Part I – Neural Beginnings \& Learning Mechanisms (1943–1990)}

The period from 1943 to 1990 established the core learning mechanisms and architectural primitives of neural computation. Rosenblatt's perceptron introduced the first learning algorithm capable of adjusting connection weights based on training examples, demonstrating that networks could learn from data rather than being manually programmed. Minsky and Papert's analysis revealed fundamental limitations of single-layer networks, motivating the search for more expressive architectures.

The integration of reinforcement learning foundations during this era proved essential for modern large language model training. Bellman's dynamic programming framework formalized sequential decision-making under optimal control, introducing value functions and the principle of optimality. Watkins's Q-learning algorithm provided a practical method for learning optimal action-value functions through temporal-difference updates, establishing the algorithmic basis for policy optimization that would later enable reinforcement learning from human feedback.

Hopfield's recurrent networks demonstrated how feedback connections enable associative memory and pattern completion. The backpropagation algorithm, formalized by Rumelhart, Hinton, and Williams, provided the computational tool for training multi-layer networks through gradient descent on differentiable error functions. This breakthrough enabled networks to learn internal representations across multiple layers of abstraction, transforming neural networks from theoretical curiosities into practical function approximators.

\textbf{Papers in this section:}
\begin{itemize}
    \item \textbf{Rosenblatt (1958)}: The Perceptron---first learning algorithm for neural networks based on error-driven weight updates.
    \item \textbf{Minsky \& Papert (1969)}: Perceptrons---rigorous analysis revealing limitations of single-layer networks.
    \item \textbf{Bellman (1957)}: Dynamic Programming---formalized sequential decision-making and value functions for optimal control.
    \item \textbf{Hopfield (1982)}: Neural Networks and Physical Systems---demonstrated associative memory through recurrent network dynamics.
    \item \textbf{Rumelhart, Hinton \& Williams (1986)}: Learning Representations by Back-Propagating Errors---gradient-based learning for multi-layer networks.
    \item \textbf{Watkins (1989/1992)}: Q-Learning---temporal-difference algorithm for learning optimal action-value functions.
\end{itemize}

\section*{Part IV – The Transformer Era and Pretraining (2017–2019)}

The transformer architecture introduced by Vaswani et al. represents a fundamental departure from recurrent processing. Multi-head self-attention mechanisms enable direct pairwise interactions between all sequence positions, eliminating the sequential bottleneck inherent in recurrent architectures. Position-invariant attention combined with learned positional encodings provides both parallelizability and the capacity to model long-range dependencies.

This era witnessed the critical integration of reinforcement learning techniques with large-scale language modeling. Schulman et al.'s Proximal Policy Optimization algorithm provided a stable and sample-efficient method for policy gradient optimization, balancing exploration with exploitation through clipped surrogate objectives. Christiano et al. demonstrated that human preferences over trajectory pairs could serve as reward signals for reinforcement learning, eliminating the need for hand-crafted reward functions. This work established reinforcement learning from human feedback as a paradigm for aligning model behavior with human values.

Espeholt et al.'s IMPALA architecture addressed the challenges of distributed reinforcement learning at scale through importance-weighted actor-learner separation. The V-trace off-policy correction enables stable learning from trajectories generated by lagged policies, allowing massive parallelization across thousands of actors.

The shift from task-specific architectures to general-purpose pretraining emerged as a dominant paradigm. ELMo demonstrated that contextualized word representations learned from language modeling transfer effectively to downstream tasks. BERT's bidirectional pretraining through masked language modeling and GPT's autoregressive approach established complementary methodologies for learning universal text representations. These models revealed that massive self-supervised pretraining on unlabeled text captures linguistic structure, world knowledge, and reasoning capabilities transferable across diverse applications.

\textbf{Papers in this section:}
\begin{itemize}
    \item \textbf{Vaswani et al. (2017)}: Attention Is All You Need---introduced transformer architecture with multi-head self-attention.
    \item \textbf{Schulman et al. (2017)}: Proximal Policy Optimization---stable policy gradient algorithm with clipped objectives.
    \item \textbf{Christiano et al. (2017)}: Deep RL from Human Preferences---reinforcement learning from preference comparisons without reward engineering.
    \item \textbf{Espeholt et al. (2018)}: IMPALA---scalable distributed reinforcement learning with off-policy correction.
    \item \textbf{Peters et al. (2018)}: ELMo---contextualized word representations from bidirectional language models.
    \item \textbf{Howard \& Ruder (2018)}: ULMFiT---transfer learning through language model fine-tuning.
    \item \textbf{Devlin et al. (2018)}: BERT---bidirectional pretraining via masked language modeling.
    \item \textbf{Radford et al. (2018)}: GPT-1---generative pretraining with autoregressive language modeling.
\end{itemize}

\section*{Part VII – Open Models \& Advanced Alignment (2023–2024)}

The release of openly accessible foundation models democratized research and development in large language models. Meta's LLaMA series demonstrated that smaller, efficiently trained models could achieve performance competitive with significantly larger proprietary systems when trained on high-quality data with appropriate compute budgets.

Alignment techniques evolved beyond reinforcement learning from human feedback. Rafailov et al.'s Direct Preference Optimization reformulated the RLHF objective to eliminate the explicit reward model and policy gradient optimization, deriving a simpler classification loss directly from preference data. This approach reduces training complexity while maintaining alignment quality.

Lee et al.'s work on Reinforcement Learning from AI Feedback demonstrated that large language models themselves could provide preference labels at scale, reducing dependence on costly human annotation. Yuan et al. extended this concept with self-rewarding language models, where the model iteratively improves both its generation and evaluation capabilities through self-generated feedback loops.

Architectural innovations addressed the quadratic complexity bottleneck of attention mechanisms. Liu et al.'s Ring Attention enables processing of sequences exceeding millions of tokens through blockwise computation distributed across devices. Munkhdalai et al.'s Infini-attention integrates compressive memory into standard attention, achieving effective infinite context through a combination of local attention and long-term linear attention mechanisms.

DeepSeek-V2 introduced Multi-Head Latent Attention and refined mixture-of-experts routing, achieving superior inference efficiency through KV cache compression and auxiliary-loss-free load balancing. The Llama 3 family established new benchmarks for open models across diverse tasks, demonstrating sophisticated capabilities in reasoning, coding, and multilingual understanding.

Research into test-time compute scaling revealed that inference-time deliberation can yield performance improvements competitive with scaling model parameters. Snell et al. demonstrated that allocating additional computation during inference through techniques like best-of-N sampling and process-based verification enables smaller models to approach or exceed the performance of larger models on challenging reasoning tasks.

\textbf{Papers in this section:}
\begin{itemize}
    \item \textbf{Touvron et al. (2023)}: LLaMA---openly accessible foundation models trained with compute-efficient scaling.
    \item \textbf{Rafailov et al. (2023)}: Direct Preference Optimization---alignment through preference classification without explicit rewards.
    \item \textbf{Lee et al. (2023)}: RLAIF---reinforcement learning from AI-generated preference labels.
    \item \textbf{Liu et al. (2023)}: Ring Attention---blockwise attention enabling near-infinite context through distributed computation.
    \item \textbf{Dettmers et al. (2023)}: QLoRA---parameter-efficient finetuning through quantization.
    \item \textbf{Munkhdalai et al. (2024)}: Infini-attention---compressive memory for effective infinite context.
    \item \textbf{Yuan et al. (2024)}: Self-Rewarding Language Models---iterative self-improvement through self-generated feedback.
    \item \textbf{Touvron et al. (2024)}: The Llama 3 Herd---open models achieving frontier capabilities across diverse tasks.
    \item \textbf{Shao et al. (2024)}: DeepSeekMath---mathematical reasoning through Group Relative Policy Optimization.
    \item \textbf{Zhu et al. (2024)}: DeepSeek-V2---Multi-Head Latent Attention and auxiliary-loss-free mixture-of-experts.
    \item \textbf{Mistral AI (2024)}: Mixtral---sparse mixture-of-experts achieving strong performance with selective activation.
    \item \textbf{Snell et al. (2024)}: Scaling Test-Time Compute---inference-time deliberation as alternative to parameter scaling.
\end{itemize}
