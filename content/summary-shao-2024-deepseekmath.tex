% Paper Summary: DeepSeekMath (Shao et al., 2024)

\begin{papersummary}{2024}{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}{Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo}{DeepSeekMath demonstrated that focused pretraining on mathematical corpora combined with reinforcement learning from tool-integrated reasoning significantly improves mathematical problem-solving, achieving competitive performance with GPT-4 on mathematical benchmarks.}

\summaryheading{Key Ideas}
DeepSeekMath combined continued pretraining on mathematical corpora with group relative policy optimization using tool-integrated reasoning traces. The approach enables models to leverage computational tools (calculators, symbolic solvers) during reasoning while learning from both correctness feedback and process supervision. The work showed that domain-specific pretraining followed by RL with tool use dramatically improves mathematical capabilities beyond general pretraining alone.

\summaryheading{Follow-on Works}
Mathematical reasoning research increasingly focuses on tool integration and process supervision. The success of domain-specific continued pretraining influences training strategies. Modern approaches to complex reasoning often incorporate external tools following this paradigm.

\summaryheading{Lasting Contributions}
DeepSeekMath demonstrated that mathematical reasoning capabilities can be substantially improved through targeted pretraining and tool-augmented RL. The work validated that open models can achieve near-frontier mathematical performance through careful training. The emphasis on process supervision and tool integration influenced how researchers approach complex reasoning tasks. DeepSeekMath showed that specialized capabilities can be developed through focused training rather than requiring massive general models.

\end{papersummary}
