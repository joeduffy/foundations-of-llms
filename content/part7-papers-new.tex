% Part VII Papers: Open Models & Advanced Alignment (2023-2024)

% 2023: LLaMA
\clearpage
\phantomsection
\label{paper:touvron-2023-llama}
\addcontentsline{toc}{subsection}{[2023] LLaMA: Open and Efficient Foundation Language Models (Touvron et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/touvron-2023.pdf}

% 2023: Direct Preference Optimization (DPO)
\clearpage
\phantomsection
\label{paper:rafailov-2023-dpo}
\addcontentsline{toc}{subsection}{[2023] Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/rafailov-2023-dpo.pdf}

% 2023: RLAIF
\clearpage
\phantomsection
\label{paper:lee-2023-rlaif}
\addcontentsline{toc}{subsection}{[2023] RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (Lee et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/lee-2023-rlaif.pdf}

% 2023: Ring Attention
\clearpage
\phantomsection
\label{paper:liu-2023-ring-attention}
\addcontentsline{toc}{subsection}{[2023] Ring Attention with Blockwise Transformers for Near-Infinite Context (Liu et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/liu-2023-ring-attention.pdf}

% 2023: QLoRA
\clearpage
\phantomsection
\label{paper:dettmers-2023-qlora}
\addcontentsline{toc}{subsection}{[2023] QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/dettmers-2023.pdf}

% 2024: Infini-attention
\clearpage
\phantomsection
\label{paper:munkhdalai-2024-infini}
\addcontentsline{toc}{subsection}{[2024] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Munkhdalai et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/munkhdalai-2024-infini-attention.pdf}

% 2024: Self-Rewarding Language Models
\clearpage
\phantomsection
\label{paper:yuan-2024-self-rewarding}
\addcontentsline{toc}{subsection}{[2024] Self-Rewarding Language Models (Yuan et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/yuan-2024-self-rewarding.pdf}

% 2024: DeepSeekMath
\clearpage
\phantomsection
\label{paper:shao-2024-deepseekmath}
\addcontentsline{toc}{subsection}{[2024] DeepSeekMath: Pushing the Limits of Mathematical Reasoning (Shao et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/shao-2024-deepseekmath.pdf}

% 2024: DeepSeek-V2
\clearpage
\phantomsection
\label{paper:zhu-2024-deepseek-v2}
\addcontentsline{toc}{subsection}{[2024] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model (Zhu et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/zhu-2024-deepseek-v2.pdf}

% 2024: Mixtral of Experts
\clearpage
\phantomsection
\label{paper:jiang-2024-mixtral}
\addcontentsline{toc}{subsection}{[2024] Mixtral of Experts (Mistral AI)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/jiang-2025.pdf}

% 2024: Llama 3 Herd
\clearpage
\phantomsection
\label{paper:touvron-2024-llama3}
\addcontentsline{toc}{subsection}{[2024] The Llama 3 Herd of Models (Touvron et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/touvron-2024-llama3.pdf}

% 2024: Scaling Test-Time Compute
\clearpage
\phantomsection
\label{paper:snell-2024-test-time}
\addcontentsline{toc}{subsection}{[2024] Scaling LLM Test-Time Compute Optimally (Snell et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/snell-2024-test-time-compute.pdf}
