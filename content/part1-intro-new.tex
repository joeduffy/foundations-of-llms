The period from 1943 to 1990 established the core learning mechanisms and architectural primitives of neural computation. Rosenblatt's perceptron introduced the first learning algorithm capable of adjusting connection weights based on training examples, demonstrating that networks could learn from data rather than being manually programmed. Minsky and Papert's analysis revealed fundamental limitations of single-layer networks, motivating the search for more expressive architectures.

The integration of reinforcement learning foundations during this era proved essential for modern large language model training. Bellman's dynamic programming framework formalized sequential decision-making under optimal control, introducing value functions and the principle of optimality. Watkins's Q-learning algorithm provided a practical method for learning optimal action-value functions through temporal-difference updates, establishing the algorithmic basis for policy optimization that would later enable reinforcement learning from human feedback.

Hopfield's recurrent networks demonstrated how feedback connections enable associative memory and pattern completion. The backpropagation algorithm, formalized by Rumelhart, Hinton, and Williams, provided the computational tool for training multi-layer networks through gradient descent on differentiable error functions. This breakthrough enabled networks to learn internal representations across multiple layers of abstraction, transforming neural networks from theoretical curiosities into practical function approximators.

\textbf{Papers in this section:}
\begin{itemize}
    \item \textbf{Bellman (1957)}: Dynamic Programming---formalized sequential decision-making and value functions for optimal control.
    \item \textbf{Rosenblatt (1958)}: The Perceptron---first learning algorithm for neural networks based on error-driven weight updates.
    \item \textbf{Minsky \& Papert (1969)}: Perceptrons---rigorous analysis revealing limitations of single-layer networks.
    \item \textbf{Hopfield (1982)}: Neural Networks and Physical Systems---demonstrated associative memory through recurrent network dynamics.
    \item \textbf{Rumelhart, Hinton \& Williams (1986)}: Learning Representations by Back-Propagating Errors---gradient-based learning for multi-layer networks.
    \item \textbf{Watkins \& Dayan (1992)}: Q-Learning---temporal-difference algorithm for learning optimal action-value functions.
\end{itemize}
