The period from 2017 to 2019 marked the most significant paradigm shift in natural language processing history. The introduction of the Transformer architecture and self-supervised pretraining revolutionized how language models are built and deployed, demonstrating that massive unsupervised pretraining followed by task-specific fine-tuning could achieve unprecedented performance across diverse NLP tasks.

The architectural revolution began with \hyperref[paper:vaswani-2017]{Vaswani et al.'s Transformer (2017)}, which introduced a fully attention-based architecture that eliminated recurrence entirely. Through multi-head self-attention, the Transformer enabled massive parallelization during training while achieving superior performance to recurrent models, establishing the foundation for all subsequent large language models.

Before BERT's bidirectional approach, \hyperref[paper:peters-2018]{Peters et al.'s ELMo (2018)} demonstrated that deep bidirectional language models could produce context-dependent word representations, dramatically improving downstream task performance. This work showed that contextualized embeddings could capture polysemy and linguistic nuances that static word vectors could not.

The pretraining paradigm was established through two complementary approaches. \hyperref[paper:devlin-2018]{BERT (Devlin et al., 2018)} introduced masked language modeling to produce bidirectional representations that achieved state-of-the-art results across virtually all NLP benchmarks, while \hyperref[paper:radford-2018]{GPT (Radford et al., 2018)} demonstrated that autoregressive pretraining of Transformers could be effectively transferred to diverse downstream tasks through fine-tuning.

These advances culminated in \hyperref[paper:raffel-2019]{T5 (Raffel et al., 2019)}, which established a unified text-to-text framework treating all NLP tasks as sequence-to-sequence problems. This work demonstrated the generality of the pretraining and fine-tuning paradigm while exploring the scaling properties that would drive subsequent developments in large language models.

% Papers follow directly after the introduction

\phantomsection
\label{paper:vaswani-2017}
\addcontentsline{toc}{subsection}{[2017] Attention Is All You Need (Vaswani et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/vaswani-2017.pdf}

\phantomsection
\label{paper:peters-2018}
\addcontentsline{toc}{subsection}{[2018] Deep Contextualized Word Representations (Peters et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/peters-2018.pdf}

\phantomsection
\label{paper:devlin-2018}
\addcontentsline{toc}{subsection}{[2018] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/devlin-2018.pdf}

\phantomsection
\label{paper:radford-2018}
\addcontentsline{toc}{subsection}{[2018] Improving Language Understanding by Generative Pre-Training (Radford et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/radford-2018.pdf}

\phantomsection
\label{paper:raffel-2019}
\addcontentsline{toc}{subsection}{[2019] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/raffel-2019.pdf}