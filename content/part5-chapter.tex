The years 2019-2020 marked a crucial turning point where language models transitioned from task-specific tools to general-purpose systems exhibiting emergent capabilities. This period established that scale---in parameters, data, and compute---fundamentally changed model behavior, with larger models demonstrating qualitatively different abilities including few-shot learning and multi-task generalization.

The key insight was that language modeling at sufficient scale could serve as a universal unsupervised learning objective, producing models capable of performing diverse tasks without task-specific training. This discovery reshaped the field's understanding of intelligence and learning.

\section*{Key Advances}

\textbf{Unsupervised Multitask Learning (2019):} GPT-2 demonstrated that large language models could perform various tasks zero-shot, revealing emergent capabilities from scale and raising important questions about model deployment and safety.

\textbf{Few-Shot Learning (2020):} GPT-3 showed that sufficiently large models could learn new tasks from just a few examples in context, without updating parameters, fundamentally changing how we interact with language models.

\textbf{Scaling Laws (2020):} Kaplan et al. discovered predictable power-law relationships between model size, dataset size, compute budget, and performance, providing a scientific framework for model development.

\textbf{Efficient Attention (2020):} Choromanski et al. introduced Performers, demonstrating how to approximate attention with linear complexity, addressing the quadratic scaling limitation of Transformers.

\textbf{Retrieval Augmentation (2020):} Lewis et al. showed how to combine neural generation with non-parametric retrieval, improving factual accuracy and enabling models to access external knowledge.

\section*{Papers in This Section}

\begin{enumerate}
\item \textbf{\hyperref[paper:radford-2019]{[2019] Language Models are Unsupervised Multitask Learners (Radford et al.)}} -- Demonstrated emergent zero-shot capabilities in large language models with GPT-2.

\item \textbf{\hyperref[paper:brown-2020]{[2020] Language Models are Few-Shot Learners (Brown et al.)}} -- Introduced GPT-3 and few-shot learning, showing that scale enables in-context learning.

\item \textbf{\hyperref[paper:kaplan-2020]{[2020] Scaling Laws for Neural Language Models (Kaplan et al.)}} -- Discovered scaling laws that predict model performance from size and compute.

\item \textbf{\hyperref[paper:choromanski-2020]{[2020] Rethinking Attention with Performers (Choromanski et al.)}} -- Developed Performers for efficient attention with linear complexity.

\item \textbf{\hyperref[paper:lewis-2020]{[2020] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al.)}} -- Created retrieval-augmented generation for improved factuality and knowledge access.
\end{enumerate}

\newpage

% Papers follow directly after the introduction

\addcontentsline{toc}{subsection}{[2019] Language Models are Unsupervised Multitask Learners (Radford et al.)}
\phantomsection
\label{paper:radford-2019}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/radford-2019.pdf}

\addcontentsline{toc}{subsection}{[2020] Language Models are Few-Shot Learners (Brown et al.)}
\phantomsection
\label{paper:brown-2020}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/brown-2020.pdf}

\addcontentsline{toc}{subsection}{[2020] Scaling Laws for Neural Language Models (Kaplan et al.)}
\phantomsection
\label{paper:kaplan-2020}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/kaplan-2020.pdf}

\addcontentsline{toc}{subsection}{[2020] Rethinking Attention with Performers (Choromanski et al.)}
\phantomsection
\label{paper:choromanski-2020}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/choromanski-2020.pdf}

\addcontentsline{toc}{subsection}{[2020] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al.)}
\phantomsection
\label{paper:lewis-2020}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/lewis-2020.pdf}