The period from 2012 to 2016 represents a watershed moment in artificial intelligence, when deep learning transformed from academic curiosity into practical technology across multiple domains. Two parallel threads converged during this era: the deep learning revolution in computer vision and the emergence of attention-based sequence modeling. Together, these advances established the architectural patterns and training techniques that would enable modern large language models.

The revolution began in computer vision with \hyperref[paper:krizhevsky-2012]{AlexNet (Krizhevsky, Sutskever \& Hinton, 2012)}, whose dramatic victory in the ImageNet competition demonstrated that deep convolutional neural networks with GPU acceleration could achieve breakthrough performance on complex recognition tasks. This success proved that theoretical foundations established in previous decades could solve real-world problems when combined with sufficient computational resources and data.

Subsequent architectural innovations addressed the challenges of training increasingly deep networks. \hyperref[paper:simonyan-zisserman-2014]{VGG (Simonyan \& Zisserman, 2014)} established that network depth was crucial for performance through systematic design using repeating 3x3 convolutional components. \hyperref[paper:ioffe-szegedy-2015]{Batch normalization (Ioffe \& Szegedy, 2015)} solved training stability problems by normalizing layer inputs, enabling much deeper networks to train reliably. Finally, \hyperref[paper:he-2015-resnet]{ResNet (He et al., 2015)} introduced residual connections that eliminated the degradation problem in very deep networks, enabling training of architectures with hundreds of layers. These skip connections became a design pattern essential for transformer architectures.

In parallel, neural language processing underwent its own revolution. \hyperref[paper:kingma-ba-2014]{Adam optimizer (Kingma \& Ba, 2014)} provided adaptive learning rates that proved fundamental for training complex architectures. \hyperref[paper:sutskever-2014]{Sutskever et al. (2014)} demonstrated that deep LSTMs could achieve state-of-the-art machine translation using encoder-decoder architectures. The key innovation was the attention mechanism introduced by \hyperref[paper:bahdanau-2014]{Bahdanau et al. (2014)}, which allowed models to dynamically align source and target sequences, solving the information bottleneck inherent in fixed-length representations. \hyperref[paper:sennrich-2015]{Sennrich et al. (2015)} addressed the vocabulary problem through byte-pair encoding, enabling neural models to handle open vocabularies compositionally.

This era established the core principles that underpin modern LLMs: depth matters when properly managed, and attention mechanisms enable flexible information routing between encoder and decoder representations.
