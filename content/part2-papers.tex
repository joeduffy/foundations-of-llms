% Part II Papers: Sequence Models & Word Embeddings (1990â€“2013)

\chapter{Hochreiter \& Schmidhuber (1997)}
\section*{Long Short-Term Memory}

LSTM networks solved the vanishing gradient problem, enabling recurrent networks to learn long-term dependencies essential for language modeling.

\includepdf[pages=-,pagecommand={}]{pdfs/hochreiter-schmidhuber-1997.pdf}

\chapter{Bengio et al. (2003)}
\section*{A Neural Probabilistic Language Model}

This paper established neural language modeling and demonstrated that neural networks could learn meaningful distributed representations of words.

\includepdf[pages=-,pagecommand={}]{pdfs/bengio-2003.pdf}

\chapter{Mikolov et al. (2013)}
\section*{Efficient Estimation of Word Representations in Vector Space}

Word2vec revolutionized natural language processing by showing how to efficiently learn high-quality word embeddings from large corpora.

\includepdf[pages=-,pagecommand={}]{pdfs/mikolov-2013.pdf}

\chapter{Graves (2013)}
\section*{Generating Sequences with Recurrent Neural Networks}

Demonstrated that RNNs could generate coherent sequences including realistic handwriting, presaging the generative capabilities of modern language models.

\includepdf[pages=-,pagecommand={}]{pdfs/graves-2013.pdf}