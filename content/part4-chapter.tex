The years 2014 to 2016 witnessed a revolutionary shift in neural language processing with the introduction of attention mechanisms and sequence-to-sequence architectures. This period established the conceptual foundation for the Transformer architecture that would emerge in 2017.

Before examining the architectural innovations that defined this era, we include \hyperref[paper:kingma-ba-2014]{Kingma and Ba's Adam optimizer (2014)}, which became fundamental to training all modern neural networks. Adam's adaptive learning rates and momentum-based updates proved essential for the stable training of increasingly complex architectures.

The sequence-to-sequence paradigm emerged through \hyperref[paper:sutskever-2014]{Sutskever et al.'s demonstration (2014)} that deep LSTMs could achieve state-of-the-art results in machine translation using a simple encoder-decoder architecture. This work established that end-to-end learning could replace complex pipeline systems in sequence transduction tasks.

The key innovation of this period was the attention mechanism, introduced by \hyperref[paper:bahdanau-2014]{Bahdanau et al. (2014)}. By allowing models to dynamically align source and target sequences, attention solved the information bottleneck problem inherent in fixed-length representations. This mechanism dramatically improved translation quality while providing interpretable alignment patterns between input and output sequences.

Finally, \hyperref[paper:sennrich-2015]{Sennrich et al. (2015)} addressed the rare word problem through byte-pair encoding, enabling neural models to handle open vocabularies and morphologically rich languages. This subword approach became standard practice in all subsequent language models, allowing them to generalize to unseen words through compositional representations.

% Papers follow directly after the introduction

\phantomsection
\label{paper:kingma-ba-2014}
\addcontentsline{toc}{subsection}{[2014] Adam: A Method for Stochastic Optimization (Kingma \& Ba)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/kingma-ba-2014.pdf}

\phantomsection
\label{paper:sutskever-2014}
\addcontentsline{toc}{subsection}{[2014] Sequence to Sequence Learning with Neural Networks (Sutskever et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/sutskever-2014.pdf}

\phantomsection
\label{paper:bahdanau-2014}
\addcontentsline{toc}{subsection}{[2014] Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/bahdanau-2014.pdf}

\phantomsection
\label{paper:sennrich-2015}
\addcontentsline{toc}{subsection}{[2015] Neural Machine Translation of Rare Words with Subword Units (Sennrich et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/sennrich-2015.pdf}