The period from 2017 to 2019 marked the most significant paradigm shift in natural language processing history. The introduction of the Transformer architecture and self-supervised pretraining revolutionized how language models are built and deployed, establishing the foundation for today's large language models.

This era demonstrated that massive unsupervised pretraining followed by task-specific fine-tuning could achieve unprecedented performance across diverse NLP tasks. The shift from recurrent to attention-based architectures enabled parallelization and scaling that would prove essential for training models on internet-scale data.

\section*{Key Advances}

\textbf{Transformer Architecture (2017):} Vaswani et al. introduced the fully attention-based Transformer, eliminating recurrence and enabling massive parallelization while achieving superior performance through multi-head self-attention.

\textbf{Contextual Embeddings (2018):} Peters et al. showed with ELMo that deep bidirectional language models could produce context-dependent word representations, dramatically improving downstream task performance.

\textbf{Transfer Learning (2018):} Howard and Ruder demonstrated systematic approaches to fine-tuning pretrained language models, establishing best practices for transfer learning in NLP.

\textbf{Bidirectional Pretraining (2018):} Devlin et al. introduced BERT, showing that masked language modeling could produce bidirectional representations that achieved state-of-the-art results across virtually all NLP benchmarks.

\textbf{Autoregressive Pretraining (2018):} Radford et al. demonstrated with GPT that unsupervised pretraining of Transformers could be effectively transferred to diverse downstream tasks through fine-tuning.

\section*{Papers in This Section}

\begin{enumerate}
\item \textbf{\hyperref[paper:vaswani-2017]{[2017] Attention Is All You Need (Vaswani et al.)}} -- Introduced the Transformer architecture based entirely on attention mechanisms.

\item \textbf{\hyperref[paper:peters-2018]{[2018] Deep Contextualized Word Representations (Peters et al.)}} -- Developed ELMo, demonstrating the power of deep contextualized word representations.

\item \textbf{\hyperref[paper:devlin-2018]{[2018] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al.)}} -- Created BERT, showing bidirectional pretraining could achieve breakthrough results.

\item \textbf{\hyperref[paper:radford-2018]{[2018] Improving Language Understanding by Generative Pre-Training (Radford et al.)}} -- Introduced GPT, demonstrating effective transfer learning from unsupervised pretraining.

\item \textbf{\hyperref[paper:raffel-2019]{[2019] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al.)}} -- Developed T5, establishing the text-to-text unified framework for all NLP tasks.
\end{enumerate}

% Papers follow directly after the introduction

\phantomsection
\label{paper:vaswani-2017}
\addcontentsline{toc}{subsection}{[2017] Attention Is All You Need (Vaswani et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/vaswani-2017.pdf}

\phantomsection
\label{paper:peters-2018}
\addcontentsline{toc}{subsection}{[2018] Deep Contextualized Word Representations (Peters et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/peters-2018.pdf}

\phantomsection
\label{paper:devlin-2018}
\addcontentsline{toc}{subsection}{[2018] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/devlin-2018.pdf}

\phantomsection
\label{paper:radford-2018}
\addcontentsline{toc}{subsection}{[2018] Improving Language Understanding by Generative Pre-Training (Radford et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/radford-2018.pdf}

\phantomsection
\label{paper:raffel-2019}
\addcontentsline{toc}{subsection}{[2019] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/raffel-2019.pdf}