% Paper Summary: GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Lepikhin et al., 2020)

\begin{papersummary}{2020}{GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}{Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen}{GShard scaled Mixture-of-Experts to 600 billion parameters using automatic model parallelism, demonstrating practical training of models 100x larger than previous systems.}

\summaryheading{Key Ideas}
GShard introduced a system for training massive sparse Mixture-of-Experts models across thousands of TPU chips using automatic sharding. The paper replaced every other feed-forward layer in a Transformer with a sparsely-gated MoE layer, where each token is routed to top-2 experts from a pool of up to 2048 experts per layer. A lightweight annotation system (gshard annotations) enables developers to specify sharding intent while the compiler handles complex distributed execution. The paper demonstrated training a 600B parameter translation model, showing that MoE enables training models 100x larger than dense models with comparable compute, achieving superior translation quality.

\summaryheading{Follow-on Works}
Switch Transformers simplified routing to top-1 expert selection. GLaM achieved strong language modeling results with sparse MoE. Mixtral (\hyperref[paper:jiang-2024-mixtral]{Jiang et al., 2024}) demonstrated that open-weight MoE models could compete with much larger dense models. DeepSeek-V2 pushed efficient MoE designs further. The sharding methodology influenced systems like Megatron-LM and Alpa for distributed training. Production systems at Google and other companies now routinely use GShard-style MoE architectures.

\summaryheading{Lasting Contributions}
GShard demonstrated that Mixture-of-Experts could scale practically to hundreds of billions of parameters, bridging the gap between the original MoE paper and modern production systems. The automatic sharding approach influenced how all subsequent distributed training systems handle model parallelism. The paper's finding that sparse models outperform dense models at equivalent compute cost established MoE as a practical architecture for frontier models. The load balancing techniques and expert capacity management introduced here remain essential for training large MoE systems. GShard-style scaling enabled the next generation of language models.

\end{papersummary}
