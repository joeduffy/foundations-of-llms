\chapter*{Epilogue}
\addcontentsline{toc}{chapter}{Epilogue}

The papers in this volume trace a path from McCulloch and Pitts' binary threshold units to language models with hundreds of billions of parameters that exhibit few-shot learning, multi-step reasoning, and instruction following. The mathematical continuity of this path is worth noting: gradient-based optimization of differentiable parametric models, the core idea of backpropagation, remains the engine of every system discussed here. What changed was architecture, scale, and training methodology---not the fundamental learning algorithm.

This continuity raises a question the collected papers do not fully answer: what are the limits of this approach, and where must it be extended?

\section*{Interpretability}

The internal representations of large transformers remain poorly understood. Mechanistic interpretability---the project of identifying specific circuits and features responsible for model behaviors---has produced initial results through techniques such as activation patching, probing classifiers, and sparse autoencoders. These methods have identified attention heads that perform induction, copy suppression, and indirect object identification in small models. Scaling these analyses to frontier models with hundreds of billions of parameters remains an open problem. The difficulty is compounded by superposition: networks appear to represent far more features than they have dimensions, encoding concepts in overlapping directions that are difficult to disentangle. A mathematical theory of superposition and polysemanticity does not yet exist.

\section*{Open Architectural Problems}

Several concrete architectural limitations persist. Attention complexity is quadratic in sequence length, and while FlashAttention, sparse attention, and linear-time alternatives such as Mamba mitigate this in practice, no existing architecture achieves the combination of constant-time per-token processing, unbounded context, and the representation quality of dense self-attention. Faithful multi-step reasoning---where the model's chain of thought reliably reflects its actual computational process rather than a post-hoc rationalization---remains unverified. Current models struggle with tasks requiring systematic search, backtracking, or formal verification, suggesting that autoregressive generation may need to be augmented with explicit planning or verification mechanisms. Continual learning without catastrophic forgetting, and efficient adaptation to non-stationary data distributions, are unsolved.

\section*{Data, Compute, and Scaling}

The scaling laws documented in this volume predict continued performance improvements with increased resources, but the regime in which those laws hold is not unbounded. High-quality text data is finite: estimates suggest that publicly available text suitable for pretraining amounts to roughly $10^{13}$ tokens, a threshold that the largest training runs are approaching. Synthetic data generation, curriculum learning, and data quality filtering offer partial solutions, but whether synthetic data can substitute for natural data without distributional collapse is an open empirical question. The Chinchilla revision of the Kaplan scaling laws shifted emphasis toward data scaling, and the optimal tradeoff between pretraining compute, inference-time compute, and data quality remains an active area of investigation. Test-time scaling---allocating additional computation during inference through search, sampling, and verification---represents a promising alternative axis, but its theoretical foundations are underdeveloped.

\section*{Alignment}

Reinforcement learning from human feedback, constitutional AI, and direct preference optimization have produced language models that are substantially more helpful, harmless, and honest than their base pretrained counterparts. Whether these techniques scale to systems significantly more capable than their human overseers is unknown. Scalable oversight---the problem of supervising systems whose capabilities exceed those of any individual human evaluator---motivates research on debate, recursive reward modeling, and AI-assisted evaluation. The alignment problem is a problem of specification: translating human values, which are contextual, inconsistent, and incompletely understood, into mathematical objectives that can be optimized reliably. The tools developed in the papers collected here---reward modeling, preference learning, constitutional principles---are initial approaches, not solutions.

\section*{Conclusion}

The trajectory documented in this volume follows a recurring pattern: theoretical frameworks are proposed, computational barriers prevent their realization, and advances in hardware, algorithms, or data eventually remove those barriers---often with consequences that the original authors did not foresee. The current frontiers---interpretability, faithful reasoning, scalable alignment, efficient long-context processing---are unlikely to be exceptions.

The papers collected here define the field's vocabulary, its standard techniques, and the intellectual trajectory from which future work will depart.
