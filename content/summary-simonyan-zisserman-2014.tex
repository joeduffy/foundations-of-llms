% Paper Summary: Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan & Zisserman, 2014)

\begin{papersummary}{2014}{Very Deep Convolutional Networks for Large-Scale Image Recognition}{Karen Simonyan, Andrew Zisserman}{VGG demonstrated that network depth is a critical factor for performance, using very deep architectures with small 3x3 convolutions to achieve state-of-the-art results on ImageNet.}

\summaryheading{Key Ideas}
VGG systematically investigated the effect of network depth, showing that stacking many layers with small 3x3 convolutions outperforms fewer layers with larger filters. The architecture's simplicity—uniform 3x3 convolutions throughout—made it easy to understand and implement. VGG networks with 16-19 layers achieved significant improvements over AlexNet, demonstrating that depth is more important than filter size. The use of small receptive fields with deeper stacks provided more non-linearity while using fewer parameters than equivalent networks with larger filters.

\summaryheading{Follow-on Works}
ResNets (\hyperref[paper:he-2015-resnet]{2015}) pushed depth even further with skip connections, enabling networks with over 100 layers. The 3x3 convolution became the standard building block in computer vision. While LLMs don't use convolutions, the principle that deeper networks with more layers capture hierarchical abstractions transferred directly to Transformers. The depth scaling insight—that performance improves with more layers—influenced the design of deep Transformer models like GPT-3 with 96 layers.

\summaryheading{Lasting Contributions}
VGG established that network depth is a fundamental driver of performance, influencing all subsequent architectures. The simplicity principle—uniform small operations repeated many times—presaged Transformer design where identical blocks stack to arbitrary depth. While VGG's specific architecture is outdated, the depth-performance relationship it revealed governs modern neural network design. Contemporary LLMs achieve their capabilities through deep stacking of Transformer layers, directly inheriting VGG's lesson that depth enables hierarchical feature learning essential for complex tasks.

\end{papersummary}
