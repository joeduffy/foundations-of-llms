\chapter*{Prologue}
\addcontentsline{toc}{chapter}{Prologue}

Large language models are among the most consequential artifacts of twentieth- and twenty-first-century science. They generate coherent text, translate between languages, reason through multi-step problems, and write functioning software. Yet these capabilities did not emerge from a single invention. They rest on an intellectual lineage spanning eight decades, in which each generation of researchers confronted specific mathematical and computational barriers, resolved them, and in doing so exposed the next set of open problems.

This volume collects the papers that constitute that lineage. Reading them in sequence, one observes not a series of isolated breakthroughs but a sustained investigation into a single deep question: \textit{to what extent can the statistical structure of language, learned from data by gradient-based optimization of parametric models, give rise to general-purpose cognitive capabilities?}

\section*{Computation and Cognition}

The question has its origins in the cybernetics movement of the 1940s. McCulloch and Pitts showed that networks of binary threshold units could compute any Boolean function, establishing that cognition---or at least logical reasoning---could in principle be realized by interconnected simple processors. Rosenblatt made this concrete with the perceptron, a device that learned classification boundaries from labeled examples. When Minsky and Papert demonstrated the perceptron's representational limitations, the field entered a period of reduced funding and diminished ambition.

Recovery came through two insights. Hopfield reframed neural computation in the language of statistical physics, showing that networks with symmetric connections minimized an energy function and could thereby implement associative memory. Rumelhart, Hinton, and Williams solved the credit assignment problem: backpropagation provided an efficient algorithm for computing gradients through multilayer networks, enabling the training of architectures deep enough to learn hierarchical representations. These results established the mathematical toolkit---gradient descent, distributed representations, compositional feature hierarchies---that remains foundational to every model discussed in this volume.

\section*{Representation and Learning}

A recurring question in artificial intelligence has been whether representations should be designed by engineers or discovered from data. Early natural language processing relied on hand-crafted grammars, feature templates, and symbolic knowledge bases. The papers in this volume document the resolution of that question in favor of learned representations.

The resolution proceeded in stages. Elman's recurrent networks showed that temporal structure could be captured through feedback connections. Hochreiter and Schmidhuber's LSTM solved the vanishing gradient problem, enabling recurrent models to retain information across long sequences. Bengio's neural language model demonstrated that words could be embedded in continuous vector spaces where geometric proximity encoded semantic similarity. Mikolov's Word2Vec made such embeddings tractable at the scale of entire corpora and revealed that the resulting vector spaces supported algebraic operations corresponding to analogical reasoning.

Each of these contributions addressed a specific technical limitation of its predecessor. Together, they established that statistical learning from raw text could produce representations rivaling or exceeding those designed by domain experts---a result whose full implications became clear only when combined with sufficient model capacity and data.

\section*{Architecture and Scale}

The transformer is the central architectural contribution in this history. Vaswani and colleagues demonstrated that self-attention alone---without recurrence or convolution---could model sequential dependencies while permitting full parallelization across sequence positions. Recurrent models processed tokens sequentially, creating a bottleneck that limited both training throughput and the effective range of learned dependencies. The transformer eliminated this bottleneck and made the subsequent era of scaling possible.

The pretraining paradigm, established through ELMo, BERT, and GPT, transformed the economics of natural language processing. Rather than training task-specific models from scratch, practitioners could fine-tune general-purpose representations learned through self-supervised objectives on large corpora. When researchers scaled these pretrained transformers---first to hundreds of millions, then to hundreds of billions of parameters---they discovered that performance improved predictably according to power laws, and that sufficiently large models exhibited qualitatively new behaviors: few-shot learning, chain-of-thought reasoning, and instruction following.

These capabilities were not explicitly engineered. They arose from next-token prediction at scale, suggesting that the statistical structure of natural language encodes more about general cognition than had been previously assumed.

\section*{Organization}

This volume organizes its papers into seven chronological parts and three appendices. Each part opens with a brief introduction situating the era in its intellectual context, followed by individual paper summaries and the papers themselves in their original form. Three appendices address active research frontiers: emerging techniques in efficiency and reasoning, the foundations of autonomous agents, and system reports from production deployments.

The volume assumes familiarity with machine learning, linear algebra, and probability theory. Technical details are preserved as they appeared in the original publications.

The papers collected here document not an endpoint but an active line of investigation. The mathematical frameworks they introduce---attention, self-supervised pretraining, reinforcement learning from human feedback, mixture-of-experts routing---continue to define the research frontier. Understanding their origins equips the reader to evaluate what comes next.

Figure~\ref{fig:concept-family-tree} illustrates the genealogical relationships among the core concepts covered in this volume.

\input{content/concept-family-tree}
