The years 2014 to 2016 witnessed a revolutionary shift in neural language processing with the introduction of attention mechanisms and sequence-to-sequence architectures. This period established the conceptual foundation for the Transformer architecture that would emerge in 2017.

The key innovation was the attention mechanism, which allowed models to dynamically focus on relevant parts of input sequences when generating outputs. This solved the information bottleneck problem in encoder-decoder architectures and enabled neural networks to handle longer sequences more effectively.

\section*{Key Advances}

\textbf{Sequence-to-Sequence Learning (2014):} Sutskever et al. demonstrated that deep LSTMs could achieve state-of-the-art results in machine translation using a simple encoder-decoder architecture, establishing the sequence-to-sequence paradigm.

\textbf{Attention Mechanism (2014):} Bahdanau et al. introduced the attention mechanism, allowing models to dynamically align source and target sequences, dramatically improving translation quality and interpretability.

\textbf{Subword Units (2015):} Sennrich et al. solved the rare word problem through byte-pair encoding, enabling neural models to handle open vocabularies and morphologically rich languages effectively.

\section*{Papers in This Section}

\begin{enumerate}
\item \textbf{\hyperref[paper:kingma-ba-2014]{[2014] Adam: A Method for Stochastic Optimization (Kingma \& Ba)}} -- Introduced the Adam optimizer, fundamental to training all modern neural networks.

\item \textbf{\hyperref[paper:sutskever-2014]{[2014] Sequence to Sequence Learning with Neural Networks (Sutskever et al.)}} -- Established sequence-to-sequence learning with deep LSTMs for neural machine translation.

\item \textbf{\hyperref[paper:bahdanau-2014]{[2014] Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al.)}} -- Introduced attention mechanisms, enabling models to align and focus on relevant input parts.

\item \textbf{\hyperref[paper:he-2015]{[2015] Deep Residual Learning for Image Recognition (He et al.)}} -- Developed residual networks with skip connections, crucial for training very deep networks.

\item \textbf{\hyperref[paper:sennrich-2015]{[2015] Neural Machine Translation of Rare Words with Subword Units (Sennrich et al.)}} -- Developed subword units through byte-pair encoding for handling rare words and open vocabularies.
\end{enumerate}

% Papers follow directly after the introduction

\phantomsection
\label{paper:kingma-ba-2014}
\addcontentsline{toc}{subsection}{[2014] Adam: A Method for Stochastic Optimization (Kingma \& Ba)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/kingma-ba-2014.pdf}

\phantomsection
\label{paper:sutskever-2014}
\addcontentsline{toc}{subsection}{[2014] Sequence to Sequence Learning with Neural Networks (Sutskever et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/sutskever-2014.pdf}

\phantomsection
\label{paper:bahdanau-2014}
\addcontentsline{toc}{subsection}{[2014] Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/bahdanau-2014.pdf}

\phantomsection
\label{paper:he-2015}
\addcontentsline{toc}{subsection}{[2015] Deep Residual Learning for Image Recognition (He et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/he-2015.pdf}

\phantomsection
\label{paper:sennrich-2015}
\addcontentsline{toc}{subsection}{[2015] Neural Machine Translation of Rare Words with Subword Units (Sennrich et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/sennrich-2015.pdf}