This appendix covers the foundational papers for AI agentsâ€”systems that use large language models to reason, plan, and act autonomously in complex environments. The emergence of capable agents represents a natural evolution of language models from passive question-answering systems to active problem-solvers.

The foundational work for executable agent behavior began with \hyperref[paper:liang-2022]{Code as Policies (Liang et al., 2022)}, which pioneered the use of language models to generate executable code for robotic control, demonstrating that language models could bridge the gap between natural language instructions and precise executable commands.

Tool use capabilities were established by \hyperref[paper:schick-2023]{Toolformer (Schick et al., 2023)}, which showed how language models can learn to use external tools through self-supervised learning. This work demonstrated that agents can effectively select and use external tools, from calculators and code interpreters to web browsers and databases, requiring careful prompt engineering and output parsing.

Advanced reasoning architectures emerged through \hyperref[paper:yao-2023-tree]{Tree of Thoughts (Yao et al., 2023)}, which generalized chain-of-thought reasoning to enable deliberate problem-solving. This approach allows agents to decompose complex goals into actionable steps, reason about consequences, and adapt plans based on feedback from the environment.

These papers establish core techniques for building agents with tool use, memory systems, planning capabilities, and error recovery mechanisms necessary to tackle real-world tasks requiring sustained interaction and goal-directed behavior.

\begin{enumerate}
\item \textbf{\hyperref[paper:liang-2022]{[2022] Code as Policies: Language Model Programs for Embodied Control (Liang et al.)}} -- Pioneered the use of language models to generate executable code for robotic control.

\item \textbf{\hyperref[paper:schick-2023]{[2023] Toolformer: Language Models Can Teach Themselves to Use Tools (Schick et al.)}} -- Showed how language models can learn to use external tools through self-supervised learning.

\item \textbf{\hyperref[paper:yao-2023-tree]{[2023] Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Yao et al.)}} -- Generalized chain-of-thought reasoning to enable deliberate problem-solving.

\item \textbf{\hyperref[paper:wang-2024-codeact]{[2024] Executable Code Actions Elicit Better LLM Agents (Wang et al.)}} -- Proved that executable code actions significantly outperform text-only approaches.

\item \textbf{\hyperref[paper:yao-2022-react]{[2022] ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al.)}} -- Established the foundation by interleaving reasoning traces with actions.

\item \textbf{\hyperref[paper:shinn-2023]{[2023] Reflexion: Language Agents with Verbal Reinforcement Learning (Shinn et al.)}} -- Introduced verbal reinforcement learning for agents that learn from failures.
\end{enumerate}

\newpage

% Papers follow directly after the introduction

\section*{Core Agent Capabilities}

The development of language model agents has proceeded through several key innovations:

\textbf{Code as Policies (2022)} pioneered the use of language models to generate executable code for robotic control, demonstrating how natural language can bridge high-level task descriptions and low-level robot actions.

\textbf{Toolformer (2023)} showed how language models can learn to use external tools like calculators, calendars, and search engines through self-supervised learning, without requiring massive amounts of tool-specific training data.

\textbf{Tree of Thoughts (2023)} generalized chain-of-thought reasoning to enable deliberate problem-solving through systematic exploration of intermediate reasoning steps, dramatically improving performance on complex tasks.

\textbf{CodeAct (2024)} proved that executable code actions significantly outperform text-only approaches for agent tasks, establishing code generation as a superior action space for tool-using agents.

\textbf{ReAct (2022)} established the foundation by interleaving reasoning traces with actions, allowing models to maintain interpretable thought processes while interacting with environments.

\textbf{Reflexion (2023)} introduced verbal reinforcement learning, where agents learn from failures by reflecting on mistakes and storing insights in memory for future tasks.

\section*{Implementation Considerations}

Building effective agents requires careful attention to:

\begin{itemize}
\item \textbf{Prompt Design:} Structuring prompts to elicit appropriate reasoning and action selection
\item \textbf{Tool Integration:} Creating reliable interfaces between language models and external systems
\item \textbf{Safety Constraints:} Ensuring agents operate within defined boundaries and cannot cause harm
\item \textbf{Evaluation Metrics:} Developing benchmarks that measure real-world task completion rather than isolated capabilities
\end{itemize}

\section*{Future Directions}

The field of AI agents is rapidly evolving, with active research in:

\begin{itemize}
\item Multi-agent coordination and communication protocols
\item Long-term memory architectures for persistent learning
\item Formal verification of agent behavior and safety properties
\item Integration with robotic systems for physical world interaction
\item Constitutional frameworks for value-aligned autonomous operation
\end{itemize}

The papers in this appendix provide essential background for understanding and building modern AI agents, representing the convergence of language models with classical AI planning, reinforcement learning, and cognitive architectures.

% Papers follow directly after the introduction

\phantomsection
\label{paper:liang-2022}
\addcontentsline{toc}{subsection}{[2022] Code as Policies: Language Model Programs for Embodied Control (Liang et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/liang-2022.pdf}

\phantomsection
\label{paper:schick-2023}
\addcontentsline{toc}{subsection}{[2023] Toolformer: Language Models Can Teach Themselves to Use Tools (Schick et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/schick-2023.pdf}

\phantomsection
\label{paper:yao-2023-tree}
\addcontentsline{toc}{subsection}{[2023] Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Yao et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/yao-2023.pdf}

\phantomsection
\label{paper:wang-2024-codeact}
\addcontentsline{toc}{subsection}{[2024] Executable Code Actions Elicit Better LLM Agents (Wang et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/wang-2024.pdf}

\phantomsection
\label{paper:yao-2022-react}
\addcontentsline{toc}{subsection}{[2022] ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/yao-2022.pdf}

\phantomsection
\label{paper:shinn-2023}
\addcontentsline{toc}{subsection}{[2023] Reflexion: Language Agents with Verbal Reinforcement Learning (Shinn et al.)}
% PDF to be added: https://arxiv.org/pdf/2303.11366.pdf