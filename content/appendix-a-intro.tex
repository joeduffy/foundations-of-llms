This appendix collects recent results from 2023--2025 that address active research frontiers: alternative architectures to the transformer, mechanistic interpretability, refined scaling analysis for sparse models, and fundamental improvements to the attention mechanism itself. These papers extend the foundations established in the main volume into directions whose long-term significance is still being assessed.

\section*{Papers in This Appendix}

\begin{enumerate}
\item \textbf{Gu \& Dao (2023)} --- Mamba: selective state-space models achieving linear-time sequence modeling as an alternative to quadratic attention.

\item \textbf{Cunningham et al. (2023)} --- Sparse autoencoders for extracting interpretable features from language model activations.

\item \textbf{Frantar et al. (2024)} --- Refined scaling laws for fine-grained mixture-of-experts architectures.

\item \textbf{Yuan et al. (2025)} --- Native Sparse Attention: hardware-aligned trainable sparse attention achieving substantial speedups while matching full attention quality.

\item \textbf{Qiu et al. (2025)} --- Gated attention: a per-head sigmoid gate on attention output that eliminates attention sinks and improves training stability.
\end{enumerate}
