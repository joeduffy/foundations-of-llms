This appendix collects recent results from 2023--2024 that address active research frontiers: alternative architectures to the transformer, mechanistic interpretability, and refined scaling analysis for sparse models. These papers extend the foundations established in the main volume into directions whose long-term significance is still being assessed.

\section*{Papers in This Appendix}

\begin{enumerate}
\item \textbf{Gu \& Dao (2023)} --- Mamba: selective state-space models achieving linear-time sequence modeling as an alternative to quadratic attention.

\item \textbf{Cunningham et al. (2023)} --- Sparse autoencoders for extracting interpretable features from language model activations.

\item \textbf{Frantar et al. (2024)} --- Refined scaling laws for fine-grained mixture-of-experts architectures.
\end{enumerate}
