This appendix captures the most significant recent developments in large language model research from 2023-2024, focusing on breakthroughs in safety, alignment, interpretability, efficiency, and reasoning capabilities. These papers address fundamental challenges in deploying language models safely and effectively while demonstrating continued rapid progress in capability development.

Safety and alignment advances include \hyperref[paper:bai-2023]{Safe RLHF (Bai et al., 2023)} with advanced techniques for safe reinforcement learning from human feedback.

Interpretability breakthroughs emerged through \hyperref[paper:cunningham-2023]{sparse autoencoders (Cunningham et al., 2023)}, which reveal highly interpretable features within language models, enabling researchers to understand internal representations and computational mechanisms previously opaque.

Architectural efficiency was revolutionized by \hyperref[paper:gu-dao-2023]{Mamba (Gu \& Dao, 2023)}, which introduced state-space models achieving linear-time sequence modeling as an alternative to quadratic-complexity attention. \hyperref[paper:munkhdalai-2024-infini]{Infini-attention (Munkhdalai et al., 2024)} introduced compressive memory mechanisms for unbounded context. \hyperref[paper:frantar-2024]{Improved mixture-of-experts scaling laws (Frantar et al., 2024)} provided more efficient approaches to scaling large models.

Enhanced reasoning capabilities were demonstrated by \hyperref[paper:openai-o1-2024]{OpenAI o1 (OpenAI, 2024)}, showing significant improvements in mathematical and scientific problem-solving through advanced reasoning techniques that leverage extended inference-time computation.

\begin{enumerate}
\item \textbf{\hyperref[paper:bai-2023]{[2023] Safe RLHF: Safe Reinforcement Learning from Human Feedback (Bai et al.)}} -- Advanced techniques for safe reinforcement learning from human feedback.

\item \textbf{\hyperref[paper:cunningham-2023]{[2023] Sparse Autoencoders Find Highly Interpretable Features in Language Models (Cunningham et al.)}} -- Breakthrough methods for understanding model internals through sparse autoencoders.

\item \textbf{\hyperref[paper:munkhdalai-2024-infini]{[2024] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Munkhdalai et al.)}} -- Compressive memory mechanisms for unbounded sequence lengths.

\item \textbf{\hyperref[paper:openai-o1-2024]{[2024] OpenAI o1 System Card (OpenAI)}} -- Advanced reasoning capabilities demonstrated by systems like OpenAI o1.

\item \textbf{\hyperref[paper:gu-dao-2023]{[2023] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Gu \& Dao)}} -- Revolutionary state-space models that achieve linear-time sequence modeling.

\item \textbf{\hyperref[paper:frantar-2024]{[2024] Scaling Laws for Fine-Grained Mixture of Experts (Frantar et al.)}} -- Improved mixture-of-experts scaling laws for efficient large models.
\end{enumerate}

% Papers follow directly after the introduction

\phantomsection
\label{paper:bai-2023}
\addcontentsline{toc}{subsection}{[2023] Safe RLHF: Safe Reinforcement Learning from Human Feedback (Bai et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/bai-2023.pdf}

\phantomsection
\label{paper:cunningham-2023}
\addcontentsline{toc}{subsection}{[2023] Sparse Autoencoders Find Highly Interpretable Features in Language Models (Cunningham et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/cunningham-2023.pdf}

\phantomsection
\label{paper:munkhdalai-2024-infini}
\addcontentsline{toc}{subsection}{[2024] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Munkhdalai et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/munkhdalai-2024-infini-attention.pdf}

\phantomsection
\label{paper:openai-o1-2024}
\addcontentsline{toc}{subsection}{[2024] OpenAI o1 System Card (OpenAI)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/openai-2024-o1.pdf}

\phantomsection
\label{paper:gu-dao-2023}
\addcontentsline{toc}{subsection}{[2023] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Gu \& Dao)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/gu-2023.pdf}

\phantomsection
\label{paper:frantar-2024}
\addcontentsline{toc}{subsection}{[2024] Scaling Laws for Fine-Grained Mixture of Experts (Frantar et al.)}
\includepdf[pages=-,pagecommand={\thispagestyle{empty}}]{pdfs/frantar-2024.pdf}