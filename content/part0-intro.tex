The foundations of modern language models rest upon probabilistic frameworks developed in the early twentieth century. Andrey Markov's pioneering work on stochastic processes introduced the concept of state transitions governed by probability distributions---a mathematical abstraction that would prove fundamental to sequence modeling. Markov chains provide the theoretical basis for understanding how systems evolve through discrete states, where the probability of transitioning to the next state depends only on the current state. This memoryless property, known as the Markov assumption, underpins contemporary language modeling, where words or tokens are generated conditionally based on preceding context.

McCulloch and Pitts's 1943 formalization of artificial neurons established the computational primitive from which all modern neural architectures descend. Their binary threshold units, though limited in expressiveness, demonstrated that logical operations could be implemented through networks of simple processing elements. This work laid the conceptual groundwork for understanding computation as an emergent property of interconnected units---a paradigm that would eventually scale to the billion-parameter models of today.

\textbf{Papers in this section:}
\begin{itemize}
    \item \textbf{Markov (1906-1913)}: Markov Chains and Stochastic Processes---introduced probabilistic state transitions fundamental to sequence modeling. \textit{Note: Historical work; authoritative survey or translation to be included.}
    \item \textbf{McCulloch \& Pitts (1943)}: A Logical Calculus of Ideas---formalized artificial neurons as binary threshold units capable of logical computation.
\end{itemize}
