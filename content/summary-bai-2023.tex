% Paper Summary: Safe RLHF: Safe Reinforcement Learning from Human Feedback (Dai et al., 2023)

\begin{papersummary}{2023}{Safe RLHF: Safe Reinforcement Learning from Human Feedback}{Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang}{Safe RLHF introduced a framework for separately modeling helpfulness and harmlessness during RLHF, enabling better control over the safety-utility trade-off.}

\summaryheading{Key Ideas}
Safe RLHF decouples the objectives of helpfulness and harmlessness by training separate reward models for each dimension. The framework formulates alignment as a constrained optimization problem where helpfulness is maximized subject to safety constraints, rather than combining objectives into a single reward. The paper introduces a Lagrangian method to adaptively balance helpfulness rewards against safety costs. This separation allows finer-grained control over model behavior and clearer understanding of trade-offs between capability and safety.

\summaryheading{Follow-on Works}
Research on multi-objective RLHF continued exploring ways to balance competing alignment objectives. Constitutional AI (\hyperref[paper:bai-2022]{Bai et al., 2022}) and Direct Preference Optimization (\hyperref[paper:rafailov-2023-dpo]{Rafailov et al., 2023}) explored alternative approaches to safety alignment. Work on red-teaming and adversarial training built on safe RLHF's framework for understanding model failure modes.

\summaryheading{Lasting Contributions}
Safe RLHF's key insight---that safety and helpfulness should be modeled as separate objectives rather than combined into a single score---has influenced how researchers think about alignment. The framework provides a principled way to navigate trade-offs rather than implicitly baking them into training data. This work contributes to the broader goal of controllable alignment where practitioners can specify and adjust the balance between different desiderata. The constrained optimization perspective may become increasingly important as models become more capable and the stakes of alignment failures increase.

\end{papersummary}
