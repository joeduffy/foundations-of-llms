# Updated Paper List for "The Foundations of Large Language Models, 1943 - 2025"

## **Part I – Neural Beginnings & Learning Mechanisms (1943–1990)**

1. McCulloch & Pitts (1943) — [A Logical Calculus of the Ideas Immanent in Nervous Activity](https://www.cs.cmu.edu/~epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)
2. Rosenblatt (1958) — [The Perceptron](https://www.cse.chalmers.se/~coquand/AIP/perceptron.pdf)
3. Minsky & Papert (1969) — [Perceptrons](https://web.media.mit.edu/~minsky/papers/Perceptrons1969.pdf)
4. Hopfield (1982) — [Neural Networks and Physical Systems with Emergent Collective Computational Abilities](https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554)
5. Rumelhart, Hinton & Williams (1986) — [Learning Representations by Back-Propagating Errors](https://www.cs.toronto.edu/~fritz/absps/naturebp.pdf)

---

## **Part II – Sequence Models & Word Embeddings (1990–2013)**

6. Elman (1990) — [Finding Structure in Time](https://crl.ucsd.edu/~elman/Papers/fsit.pdf)
7. Hochreiter & Schmidhuber (1997) — [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)
8. Bengio et al. (2003) — [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
9. Mikolov et al. (2010) — [Recurrent Neural Network Based Language Model](https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf)
10. Mikolov et al. (2013) — [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
11. **NEW** Graves (2013) — [Generating Sequences with Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850.pdf)
12. Pennington et al. (2014) — [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
13. **NEW** Cho et al. (2014) — [Learning Phrase Representations using RNN Encoder-Decoder](https://arxiv.org/pdf/1406.1078.pdf)

---

## **Part III – Attention and Sequence-to-Sequence Modeling (2014–2016)**

14. Sutskever et al. (2014) — [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
15. Bahdanau et al. (2014) — [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
16. **NEW** Luong et al. (2015) — [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)
17. Sennrich et al. (2015) — [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)

---

## **Part IV – The Transformer Era and Pretraining Revolution (2017–2019)**

18. Vaswani et al. (2017) — [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
19. Peters et al. (2018) — [Deep Contextualized Word Representations (ELMo)](https://arxiv.org/pdf/1802.05365.pdf)
20. Howard & Ruder (2018) — [Universal Language Model Fine-Tuning (ULMFiT)](https://arxiv.org/pdf/1801.06146.pdf)
21. Devlin et al. (2018) — [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/pdf/1810.04805.pdf)
22. Radford et al. (2018) — [Improving Language Understanding by Generative Pretraining (GPT-1)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

---

## **Part V – Emergence and Scale (2019–2020)**

23. Radford et al. (2019) — [Language Models are Unsupervised Multitask Learners (GPT-2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
24. Brown et al. (2020) — [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/pdf/2005.14165.pdf)
25. Kaplan et al. (2020) — [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)
26. Choromanski et al. (2020) — [Rethinking Attention with Performers](https://arxiv.org/pdf/2009.14794.pdf)
27. Lewis et al. (2020) — [Retrieval-Augmented Generation for Knowledge-Intensive NLP](https://arxiv.org/pdf/2005.11401.pdf)

---

## **Part VI – Efficiency, Alignment, and Reasoning (2021–2022)**

28. Hu et al. (2021) — [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
29. Su et al. (2021) — [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf)
30. Press et al. (2021) — [ALiBi: Position Biases for Longer Sequences](https://arxiv.org/pdf/2108.12409.pdf)
31. Borgeaud et al. (2021) — [Improving Language Models by Retrieving from Trillions of Tokens (RETRO)](https://arxiv.org/pdf/2112.04426.pdf)
32. **NEW** Shazeer (2020) — [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202.pdf)
33. Fedus et al. (2022) — [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)
34. Dao et al. (2022) — [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf)
35. Ouyang et al. (2022) — [Training Language Models to Follow Instructions with Human Feedback (InstructGPT)](https://arxiv.org/pdf/2203.02155.pdf)
36. Wei et al. (2022) — [Chain-of-Thought Prompting Elicits Reasoning in Language Models](https://arxiv.org/pdf/2201.11903.pdf)
37. Bai et al. (2022) — [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf)
38. Yao et al. (2022) — [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)
39. Hoffmann et al. (2022) — [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/pdf/2203.15556.pdf)
40. **NEW** Chowdhery et al. (2022) — [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)

---

## **Part VII – Open LLMs and Modern Frontier (2023–2024)**

41. Touvron et al. (2023) — [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
42. Dettmers et al. (2023) — [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)
43. OpenAI (2023) — [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf)
44. Mistral AI (2024) — [Mixtral of Experts: Sparse Mixture of Experts Model](https://arxiv.org/pdf/2401.04088.pdf)
45. DeepMind (2024) — [Gemini 1.5 Technical Report](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)

---

## **Appendix A – Emerging Results (2023–2025)**

46. **NEW** Gu & Dao (2023) — [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752.pdf)
47. **NEW** Liu et al. (2023) — [Ring Attention with Blockwise Transformers for Near-Infinite Context](https://arxiv.org/pdf/2310.01889.pdf)
48. **VERIFIED** Yan et al. (2025) — [InftyThink: Breaking the Length Limits of Long-Context Reasoning](https://arxiv.org/pdf/2503.06692.pdf)

---

## **Appendix B – Foundations of Agents (2022–2024)**

49. **NEW** Liang et al. (2022) — [Code as Policies: Language Model Programs for Embodied Control](https://arxiv.org/pdf/2209.07753.pdf)
50. **NEW** Schick et al. (2023) — [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761.pdf)
51. **NEW** Wang et al. (2024) — [Executable Code Actions Elicit Better LLM Agents](https://arxiv.org/pdf/2402.01030.pdf)

---

## **Summary of Changes:**

### **Added Papers:**
- Graves (2013) - Sequence generation breakthrough
- Cho et al. (2014) - GRU and encoder-decoder architecture  
- Luong et al. (2015) - Global vs local attention mechanisms
- Shazeer (2020) - GLU variants including SwiGLU
- Chowdhery et al. (2022) - PaLM scaling milestone
- Mamba (2023) - Linear-time alternative to transformers
- Ring Attention (2023) - Near-infinite context solution
- Agent papers for new Appendix B

### **Removed:**
- All fictional 2025 papers except verified InftyThink

### **New Structure:**
- Appendix A: Emerging core LLM advances
- Appendix B: Agent/tool-use foundations

**Total Papers:** 51 (up from 47, but more accurate and complete)